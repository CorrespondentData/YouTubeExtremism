{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#Introdcution)\n",
    "1. a. [Configuration](#Configuration)\n",
    "1. b. [Import data](#Import-data)\n",
    "2. [Questions](#Questions)\n",
    "3. [Question #1: The producers](#Question-#1:-The-producers)\n",
    "3. a [The producers: channel info](#Channel-info)\n",
    "3. b [The producers: video info](#Video-info)\n",
    "3. c [The producers: recommendations info](#Recommendations-info)\n",
    "3. d [The producers: topics info](#Topics-info)\n",
    "4. [Question #2: The users](#2:-The-users)\n",
    "4. a [Dutch commenters on international channels](#Dutch-commenters-on-international-channels)\n",
    "4. b [Commenters of specific Dutch channels](#Commenters-of-specific-Dutch-channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is used for the analysis of information networks on YouTube and to make this analysis reproducable. I'll take you step by step through the data and analyses, trying to find angles for stories. You can use the Table of Contents to skip to the relevant parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "First do some configuration, import libraries and set paths to data. Throughout the Notebook, Python3.6 is used. I'll import all libraries at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #basically the engine for the whole analysis. \n",
    "import matplotlib.pyplot as plt #for plotting our data.\n",
    "import glob #a nice library for iterating through multiple files.\n",
    "import networkx as nx #we need this to construct and export network graphs.\n",
    "import seaborn as sns; sns.set() #for plotting\n",
    "import comment_lib #some local modules\n",
    "import csv #for reading and writing csv's when we are not using the pandas library.\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to NL data - better to set these constants in a separate config file and import them here.\n",
    "\n",
    "path = '/home/dim/Documents/projecten/extremisme/youtube/yt/YouTubeExtremism/DataCollection/output/NL/'\n",
    "\n",
    "# Set path to control group data.\n",
    "\n",
    "path_c = '/home/dim/Documents/projecten/extremisme/youtube/data/temp/bubble/right/NL/'\n",
    "\n",
    "# Set path to international right data\n",
    "\n",
    "path_i = '/home/dim/Documents/projecten/extremisme/youtube/data/temp/bubble/right/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "Types of data are channels, videos, comments, recommendations and transcripts (for topics). The data are spread over multiple csv's so we have to append them first and create one dataframe for each type of data. We'll write the results to a csv file you can import later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import videofiles into one dataframe.\n",
    "parse_dates = ['video_published']\n",
    "filename = 'videos_nl*.csv'\n",
    "\n",
    "all_files = glob.glob(path + filename)\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, parse_dates=parse_dates)\n",
    "    list_.append(df)\n",
    "videos = pd.concat(list_, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.to_csv(path + 'all_nl_videos.csv', index=None)\n",
    "del videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comment files into one dataframe.\n",
    "\n",
    "parse_dates = ['comment_time']\n",
    "filename = 'comments_nl*.csv'\n",
    "\n",
    "all_files = glob.glob(path + filename)\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, parse_dates=parse_dates)\n",
    "    list_.append(df)\n",
    "comments = pd.concat(list_, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv(path + 'all_nl_comments.csv', index=None)\n",
    "del comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import recommendations files into one dataframe.\n",
    "\n",
    "parse_dates = ['publishedAt']\n",
    "filename = 'recommendations*.csv'\n",
    "\n",
    "all_files = glob.glob(path + \"recommendations*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, parse_dates=parse_dates)\n",
    "    list_.append(df)\n",
    "recommendations = pd.concat(list_, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.to_csv(path + 'all_nl_recommendations.csv', index=None)\n",
    "del recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transcripts files into one dataframe.\n",
    "\n",
    "filename = 'transcripts*.csv'\n",
    "\n",
    "all_files = glob.glob(path + filename)\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "transcripts = pd.concat(list_, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts.to_csv(path + 'all_nl_transcripts.csv', index=None)\n",
    "del transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from control group\n",
    "\n",
    "We want to compare the results from the NL right information network with other networks. For instance, we want to compare the behavior of certain political parties (in the Netherlands Forum voor Democratie and the PVV) with centre and left wing parties. \n",
    "\n",
    "TODO: make a list of relevant control channels. At least PvdD, SP, DENK, PvdA, D66, GroenLinks, ChristenUnie, VVD, CDA. Other candidates: Zondag met Lubach, De Nieuwe Maan, ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import channels\n",
    "\n",
    "channels_control = pd.read_csv(path + 'channels_nl_controlgroup_politiek.csv')\n",
    "\n",
    "# Import videos\n",
    "\n",
    "videos_control = pd.read_csv(path + 'videos_nl_controlgroup_politiek.csv')\n",
    "\n",
    "#import comments still TODO:\n",
    "\n",
    "#import recommendations still TODO:\n",
    "\n",
    "#import transcripts still TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "So we're all set up. Before we dive in, what kind of questions do we want to answer? \n",
    "\n",
    "1. What kind of content is being watched by Dutch viewers? (The producers)\n",
    "2. Who is commenting on the videos in the far right information network? How are commenters interacting? (The users)\n",
    "3. How do political parties compare in terms of content, marketing strategies and reach? (Comparisson and strategies)\n",
    "4. How does the far right information network compare to other information networks (like far left and center)? (Whataboutism)\n",
    "5. What content is harmful, hateful, or illegal, in other words, when are lines being crossed? (Morality, the Platform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #1: The producers\n",
    "\n",
    "For this we need:\n",
    "1. Statistics on videos, channels and recommendations.\n",
    "2. Topics of videos (by tags or through topic modelling)\n",
    "\n",
    "Let's start by looking at the channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel info\n",
    "\n",
    "Let's plot some channel data, like number of subscriptions and views over time. That will give us a sense of how certain channels are developing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the channel data into a dataframe.\n",
    "\n",
    "channels = pd.read_csv(path + 'channels_nl_right.csv')\n",
    "\n",
    "# Take a subset of the channel data.\n",
    "\n",
    "stats = channels[['channel_title', \n",
    "                  'channel_description', \n",
    "                  'channel_subscribercount',\n",
    "                  'channel_viewcount', \n",
    "                  'channel_videocount']]\n",
    "\n",
    "stats = stats.sort_values(by='channel_subscribercount', ascending=False)\n",
    "stats.set_index(\"channel_title\",drop=True,inplace=True)\n",
    "\n",
    "# Create matplotlib figure.\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) \n",
    "\n",
    "# Create matplotlib axes.\n",
    "\n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "# Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "ax2 = ax.twinx() \n",
    "\n",
    "# Set a width for a bar chart.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "# Configure the bar chart.\n",
    "\n",
    "stats.channel_subscribercount.plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "stats.channel_viewcount.plot(kind='bar', color='blue', ax=ax2, width=width, position=0, legend=True, grid=True)\n",
    "ax.set_ylabel('subscribers')\n",
    "ax2.set_ylabel('views')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, because there are two y-axis. The left is subscribers, the right is viewcount. \n",
    "\n",
    "That said though, there are some takeaways and questions:\n",
    "1. PVVpers has 0 subscribers. That means that the subscribercount has not been given on the channel page. They have a lot of viewers. Still more than Forum voor Democratie, but FvD is catching up. PVV is much older.\n",
    "2. Some channels generate a lot of views, like Laurens, Rafiek de Bruin, Leukste YouTube Fragmenten, Deweycheatumhowe and the LvKrijger. Most of them are very pro FvD and pro PVV.\n",
    "3. FvD has relatively many subscribers (they rank 2nd), but not that many views (relitavely, they rank 4th). Did they buy subscribers? \n",
    "4. Why did Rossen remove all his videos? He was quite popular.\n",
    "5. If we look at FvD more broadly and take affiliated channels into consideration, FvD is very big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show channel development over time\n",
    "\n",
    "Socialblade.com provides a range of statistics on YouTube channels, like daily views and subscription info. I've run the list of channels through [socialblade.com](https://www.socialblade.com). I want to try to get a sense of the the growth of the far right network in recent years, maybe in a bubble flow chart. It would make a great comparisson with other information networks on YouTube. We can use four axes for that:\n",
    "- x = monthly_views\n",
    "- y = monthly_subscriptions\n",
    "- z = monthly_comments (z is size of the bubble)\n",
    "- plus time\n",
    "\n",
    "The only constraint is that the oldest data is from early 2015, so it's not that old.\n",
    "\n",
    "I'll prepare the data for use in [gapminder](https://www.gapminder.org/tools/), an easy way to explore this kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from socialblade\n",
    "\n",
    "channel_history = pd.read_csv(path_i + 'other_platforms/social_blade_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the dates and values of two columns: daily views and total subs\n",
    "\n",
    "import re\n",
    "pattern = re.compile('(\\d{4}-\\d{2}-\\d+,\\d+)')\n",
    "\n",
    "# And create two new columns with lists of dates and values found\n",
    "\n",
    "channel_history['daily_views'] = channel_history['Date_Daily_Views'].str.findall(pattern)\n",
    "channel_history['daily_subs'] = channel_history['Date_Total_Subs'].str.findall(pattern)\n",
    "\n",
    "# Stack them, so all the dates and values are linked to the channels and\n",
    "# we are getting rid of the messy lists.\n",
    "\n",
    "daily_views = channel_history.set_index('User') \\\n",
    "            .daily_views.apply(pd.Series) \\\n",
    "            .stack() \\\n",
    "            .reset_index(level=-1, drop=True) \\\n",
    "            .reset_index()\n",
    "\n",
    "# Extract the values columns for views and subscriptions (subs)\n",
    "\n",
    "daily_views['date'], daily_views['views'] = daily_views[0].str.split(',', 1).str\n",
    "daily_views = daily_views[['User', 'date', 'views']]\n",
    "daily_views = daily_views.rename(columns = {'User': 'channel_id'})\n",
    "\n",
    "daily_subs = channel_history.set_index('User') \\\n",
    "            .daily_subs.apply(pd.Series) \\\n",
    "            .stack() \\\n",
    "            .reset_index(level=-1, drop=True) \\\n",
    "            .reset_index()\n",
    "\n",
    "daily_subs['date'], daily_subs['subs'] = daily_subs[0].str.split(',', 1).str\n",
    "daily_subs = daily_subs[['User', 'date', 'subs']]\n",
    "daily_subs = daily_subs.rename(columns = {'User': 'channel_id'})\n",
    "\n",
    "# And bring it all together in a dataframe called daily_stats\n",
    "\n",
    "daily_stats = pd.merge(daily_subs, daily_views,  how='left', left_on=['channel_id', 'date'], right_on = ['channel_id', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to add some data, first the channel data (like channel_title, etc.)\n",
    "\n",
    "# Import the channel data\n",
    "\n",
    "channels_int = pd.read_csv(path_i + 'channels_right.csv')\n",
    "\n",
    "# And merge them with daily_stats\n",
    "\n",
    "int_channel_daily_stats = pd.merge(daily_stats, channels_int, on='channel_id', how='left')\n",
    "\n",
    "# Drop empty values\n",
    "\n",
    "int_channel_daily_stats = int_channel_daily_stats.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the average (mean) views and subs per year, month and year_month\n",
    "\n",
    "# The date is not recognized as a date\n",
    "\n",
    "int_channel_daily_stats['date'] = pd.to_datetime(int_channel_daily_stats['date'])\n",
    "\n",
    "# Get year, month and year_month (yyyy-mm format)\n",
    "\n",
    "int_channel_daily_stats['year'] = int_channel_daily_stats['date'].dt.year\n",
    "int_channel_daily_stats['month'] = int_channel_daily_stats['date'].dt.month\n",
    "int_channel_daily_stats['yearmonth'] = int_channel_daily_stats['date'].dt.to_period('M')\n",
    "\n",
    "# The values of subs and views are not integers yet, which will get us into trouble later on\n",
    "\n",
    "int_channel_daily_stats['subs'] = int_channel_daily_stats['subs'].astype('int')\n",
    "int_channel_daily_stats['views'] = int_channel_daily_stats['views'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then it's time to get the comments and average out the comments per month\n",
    "# (or should we sum them? Let's try both)\n",
    "\n",
    "# Import the comments usering an iterator (the comments file is 4.5GB)\n",
    "\n",
    "columns = ['video_id', \n",
    "           'comment_id', \n",
    "           'comment_id2', \n",
    "           'author_display_name',\n",
    "           'author_image',\n",
    "           'author_channel_url',\n",
    "           'author_channel_id',\n",
    "           'comment_text',\n",
    "           'number_of_replies',\n",
    "           'comment_date'\n",
    "          ]\n",
    "cols_to_keep = ['video_id', 'comment_date']\n",
    "\n",
    "comments_we_need = pd.concat([x.loc[:, cols_to_keep] for x in pd.read_csv(path_i + 'comments_right.csv', names=columns, chunksize=20000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel data to comments_we_need\n",
    "\n",
    "videos = pd.read_csv(path_i + 'videos_right.csv', low_memory=False, index_col=None)\n",
    "comments_channels_to_clean = pd.merge(comments_we_need, videos[['video_id', 'video_channel_title']], on='video_id').dropna()\n",
    "\n",
    "# And make some room in memory\n",
    "\n",
    "del videos\n",
    "del comments_we_need\n",
    "\n",
    "# Parse some dates.\n",
    "\n",
    "comments_channels_to_clean['comment_date'] = pd.to_datetime(comments_channels_to_clean['comment_date'])\n",
    "comments_channels_to_clean['year'] = comments_channels_to_clean['comment_date'].dt.year\n",
    "comments_channels_to_clean['month'] = comments_channels_to_clean['comment_date'].dt.month\n",
    "comments_channels_to_clean['yearmonth'] = comments_channels_to_clean['comment_date'].dt.to_period('M')\n",
    "\n",
    "# And clean it up a bit.\n",
    "\n",
    "comments_channels_to_clean = comments_channels_to_clean.rename(columns = {'video_channel_title': 'channel_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for merging - the code is still quite messy\n",
    "# TODO: Clean it up a bit and make it more pythonic. Maybe write a function.\n",
    "\n",
    "int_channel_daily_stats = int_channel_daily_stats[['channel_title', \n",
    "                                                   'subs', 'views', \n",
    "                                                   'yearmonth', \n",
    "                                                   'year', \n",
    "                                                   'month']]\n",
    "\n",
    "comments_channels_to_clean = comments_channels_to_clean.groupby([comments_channels_to_clean.channel_title, \n",
    "                                                                 comments_channels_to_clean.yearmonth ]) \\\n",
    "                                                                .agg('count')\n",
    "\n",
    "comments_channels_to_clean = comments_channels_to_clean \\\n",
    "                            .rename(columns = {'video_id':'comments'}) \\\n",
    "                            .reset_index()\n",
    "\n",
    "comments_channels_to_clean = comments_channels_to_clean[['channel_title', 'yearmonth', 'comments']]\n",
    "\n",
    "merged_comments = pd.merge(int_channel_daily_stats, \n",
    "                           comments_channels_to_clean, \n",
    "                           on=['channel_title', 'yearmonth'], \n",
    "                           how='left')\n",
    "\n",
    "subset_for_graph = int_channel_daily_stats[['channel_id', \n",
    "                                            'channel_title', \n",
    "                                            'yearmonth', \n",
    "                                            'subs', \n",
    "                                            'views']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And bring it all finally together.\n",
    "\n",
    "df1 = pd.melt(merged_comments, id_vars=['channel_title', \n",
    "                                        'yearmonth', \n",
    "                                        'month', \n",
    "                                        'year'])\n",
    "\n",
    "df2 = df1.groupby(['channel_title',\n",
    "                   'yearmonth', \n",
    "                   'month', \n",
    "                   'year', \n",
    "                   'variable']) \\\n",
    "                    .mean()\\ \n",
    "                    .unstack(['yearmonth'])\n",
    "\n",
    "# Write it to csv for use in Gapminder\n",
    "\n",
    "df2.to_csv(path + 'for_viz/forgapminder.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load videos.\n",
    "\n",
    "videos = pd.read_csv(path + 'all_nl_videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a year column.\n",
    "\n",
    "videos['video_upload_year'] = pd.DatetimeIndex(videos['video_published']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot views and uploads per year.\n",
    "\n",
    "uploads_per_year = videos.groupby(['video_upload_year']).size()\n",
    "views_per_year = videos.groupby(['video_upload_year'])['video_view_count'].agg('sum')\n",
    "\n",
    "fig = plt.figure(figsize=(10,5)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "uploads_per_year.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of videos published')\n",
    "ax.set_xlabel('year')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# First the uploads per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "width = 0.4\n",
    "\n",
    "views_per_year.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of views')\n",
    "ax.set_xlabel('year')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Then the views per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting:\n",
    "\n",
    "1. In 2018 more videos were uploaded, but they've gotten significantly lesser views. It could be that older video's are still getting views. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare some channels.\n",
    "\n",
    "channel1 = 'Forum Democratie' #fill in the channels you want to compare\n",
    "channel2 = 'PVVpers'\n",
    "\n",
    "filtered = videos.loc[(videos['video_channel_title'] == channel1) | \\\n",
    "                      (videos['video_channel_title'] == channel2)\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the number of published videos per year.\n",
    "\n",
    "ax = filtered.groupby(['video_upload_year','video_channel_title'])['video_channel_title'] \\\n",
    "        .count().unstack(1).plot.bar(title=\"Number of uploaded videos\", figsize=(10,5), grid=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of uploads')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now look at the viewcount per year.\n",
    "\n",
    "ax = filtered.groupby(['video_upload_year', 'video_channel_title'])['video_view_count'] \\\n",
    "        .sum().unstack(1).plot.bar(title=\"Number of views per year\", figsize=(10,5), grid=True, legend=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of views')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the comment count per year.\n",
    "\n",
    "ax = filtered.groupby(['video_upload_year', 'video_channel_title'])['video_comment_count'] \\\n",
    "        .sum().unstack(1).plot.bar(title=\"Number of comments per year\", figsize=(10,5), grid=True, legend=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of comments')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some takeaways from the comparison of PVV and FvD:\n",
    "1. FvD is winning on YouTube, by a large margin.\n",
    "2. They are much more active in uploading content\n",
    "3. That content reaches a larger audience. TODO: to be sure we need to look at the average views per video.\n",
    "4. It's clear that there is much more debate, or at least more comments on FvD than on PVV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the mean of viewcount per video\n",
    "\n",
    "ax = filtered.groupby(['video_upload_year', 'video_channel_title'])['video_view_count'] \\\n",
    "        .mean().unstack(1).plot.bar(title=\"Number of views per year\", figsize=(10,5), grid=True, legend=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of views')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still want to have a good look at it, but it seems that the mean views per video is about the same. This could mean that FvD is simply uploading a lot more content, that doesn't gather many views, while PVV is uploading not so much content, but what is uploaded is peforming better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations info\n",
    "\n",
    "The channel data for the recommendations is missing, so we need to add them and merge them with the recommendations and the videos. While we are at it, let's use a simpler variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recommendations.\n",
    "\n",
    "recommendations = pd.read_csv(path + 'all_nl_recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize field names (this will be fixed in future versions of the DataCollection library)\n",
    "\n",
    "recommendations = recommendations.rename(columns={'channelId':'channel_id',\n",
    "                                                  'description': 'target_channel_description',\n",
    "                                                  'publishedAt': 'target_video_published',\n",
    "                                                  'targetVideoId': 'target_video_id',\n",
    "                                                  'title': 'target_video_title',\n",
    "                                                  'videoId': 'source_video_id' })\n",
    "\n",
    "video_channels = pd.merge(videos, channels, on='channel_id', how='left')\n",
    "\n",
    "video_channels = video_channels.rename(columns={'channel_id': 'source_channel_id',\n",
    "                                                'video_category_id': 'source_video_category_id',\n",
    "                                                'video_channel_title': 'source_channel_title',\n",
    "                                                'video_description': 'source_video_description',\n",
    "                                                'video_id': 'source_video_id',\n",
    "                                                'video_published': 'source_video_published',\n",
    "                                                'video_tags': 'source_video_tags',\n",
    "                                                'video_title': 'source_video_title',\n",
    "                                                'video_view_count': 'source_video_viewcount',\n",
    "                                                'channel_topic_ids': 'source_channel_topic_ids',\n",
    "                                                'channel_subscribercount': 'source_channel_subscribercount'})\n",
    "\n",
    "recs_chans = pd.read_csv(path + 'recs_chans.csv')\n",
    "recs_channels_for_merge = pd.merge(recommendations, recs_chans, on='channel_id', how='left')\n",
    "\n",
    "recs_channels_for_merge = recs_channels_for_merge.rename(columns={'channel_id': 'target_channel_id',\n",
    "                                                                 'channel_title': 'target_channel_title',\n",
    "                                                                 'channel_description': 'target_channel_description',\n",
    "                                                                 'channel_viewcount': 'target_channel_viewcount',\n",
    "                                                                 'channel_subscribercount': 'target_channel_subscribercount',\n",
    "                                                                 'channel_topic_ids': 'target_channel_topic_ids'})\n",
    "\n",
    "recs = pd.merge(recs_channels_for_merge, video_channels, on='source_video_id', how='left')\n",
    "\n",
    "recs = recs.drop(['channel_country_x',\n",
    "                  'channel_default_language_x',\n",
    "                  'channel_uploads_x',\n",
    "                  'channel_commentcount_x',\n",
    "                  'channel_videocount_x',\n",
    "                  'channel_topic_categories_x',\n",
    "                  'channel_branding_keywords_x',\n",
    "                  'video_comment_count',\n",
    "                  'video_default_language',\n",
    "                  'video_dislikes_count',\n",
    "                  'video_duration',\n",
    "                  'video_likes_count',\n",
    "                  'video_upload_year',\n",
    "                  'channel_title',\n",
    "                  'channel_viewcount',\n",
    "                  'channel_country_y',\n",
    "                  'channel_commentcount_y',\n",
    "                  'channel_uploads_y',\n",
    "                  'channel_viewcount',\n",
    "                  'channel_branding_keywords_y',\n",
    "                  'channel_topic_categories_y',\n",
    "                  'channel_videocount_y',\n",
    "                  'video_topic_categories',\n",
    "                  'video_topic_ids',\n",
    "                  'channel_default_language_y',\n",
    "                  'channel_description'\n",
    "                 ], axis=1)\n",
    "\n",
    "recs = recs.rename(columns={'source_video_title_y': 'source_video_title'})\n",
    "\n",
    "\n",
    "cols = ['source_video_id',\n",
    "        'source_video_title',\n",
    "        'source_video_description',\n",
    "        'source_video_published',\n",
    "        'source_video_tags',\n",
    "        'source_video_viewcount',\n",
    "        'source_channel_id',\n",
    "        'source_video_category_id',\n",
    "        'source_channel_title',\n",
    "        'source_channel_subscribercount',\n",
    "        'source_channel_topic_ids',\n",
    "        'target_video_id',\n",
    "        'target_video_title',\n",
    "        'target_channel_id',\n",
    "        'target_channel_description',\n",
    "        'target_video_published',\n",
    "        'target_channel_title',\n",
    "        'target_channel_description',\n",
    "        'target_channel_viewcount']\n",
    "\n",
    "recs = recs[cols]\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample of the data.\n",
    "\n",
    "recs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many videos and recommendations are in this set?\n",
    "len(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick reminder of the channels.\n",
    "\n",
    "recs.source_channel_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a channel\n",
    "\n",
    "chan = 'Erkenbrand Kanaal' #fill in a channel here\n",
    "\n",
    "#and filter\n",
    "\n",
    "filtered_recs = recs[recs['source_channel_title'] == chan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the related channels of the videos and how often YouTube has assigned these related channels.\n",
    "\n",
    "filtered_recs.target_channel_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to gexf file, for analysis in Gephi.\n",
    "\n",
    "G = nx.from_pandas_edgelist(recs, source='source_channel_title', target='target_channel_title')\n",
    "nx.write_gexf(G, path + 'nl_graphs/nl_recommendations.gexf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a video from the selected channel.\n",
    "\n",
    "vid = 'Conference interview with Millennial Woes [2018 ENGLISH]' #change this to another video title\n",
    "\n",
    "filtered_rec_vids = filtered_recs[filtered_recs['source_video_title'] == vid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pick another video of this list of videps of selected channel.\n",
    "\n",
    "filtered_recs.source_video_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube thinks that these videos are related to the selected videos.\n",
    "\n",
    "filtered_rec_vids.target_video_title.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics info\n",
    "\n",
    "This still needs some work. The tags are malformed, and I'm not so sure about the quality of the transcripts. I would say this doens't have a high priority, so I'll leave this to later and focus on the users first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tags, first link tags to videos and clean them up a bit\n",
    "\n",
    "vidtags = videos[['video_id', 'video_title', 'video_tags']]\n",
    "\n",
    "vidtags = vidtags.video_tags.str.split(', | #', expand=True)\\\n",
    "    .merge(vidtags, left_index = True, right_index = True)\\\n",
    "    .drop(['video_tags'], axis=1)\\\n",
    "    .melt(id_vars = ['video_id'], value_name = \"tags\") \\\n",
    "    .dropna() \\\n",
    "    .drop(['variable'], axis=1)\n",
    "\n",
    "vidtags['tags'].replace(regex=True,inplace=True,to_replace=r\"'|\\[|\\]|#|\\\"\",value=r'')\n",
    "\n",
    "vidtags.tags = vidtags.tags.str.lower()\n",
    "\n",
    "vidtags = vidtags[vidtags.tags != 'not set']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for certain tags\n",
    "\n",
    "vidtags = vidtags[vidtags['tags'].str.contains(\"rassenhaat\")]\n",
    "vidtags.tags.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then get the video data with these tags.\n",
    "\n",
    "vidtags = pd.merge(vidtags, videos, on='video_id', how='left')\n",
    "vidtags[['video_id', 'tags', 'video_channel_title_x']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? Something is wrong here. I get a description in the tags, so this still needs some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2: The users\n",
    "\n",
    "Who is commenting on the videos in the far right information network? How are commenters interacting? (The users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comment data\n",
    "\n",
    "comments = pd.read_csv(path + 'all_nl_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many comments do we have?\n",
    "\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the hardcore commenters in the Dutch network\n",
    "\n",
    "First I'm interested in some statistics to get to the hardcore commenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique author names\n",
    "\n",
    "comments.author_display_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique author id's\n",
    "\n",
    "comments.author_channel_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to be a bit careful, because there are more unique id's than names, which is kind of obvious.\n",
    "\n",
    "Let's start with adding more information to the comment data, so we can select and filter some channels. We can do this by adding the video data to the comment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_comment_sphere = pd.merge(comments, videos, on='video_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the merge was succesful.\n",
    "\n",
    "len(nl_comment_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the available channels?\n",
    "\n",
    "nl_comment_sphere.video_channel_title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* Erkenbrand is missing. The channel doens't have a lot of comments, but it has some. We'll need to add Erkenbrand, because it can be important. There are also some other channels I would like to add, like Nederlands Falen, Linkse Moskee and some other.\n",
    "* For the purposes of our research, I'm going to filter out a couple of channels that are run by Dutch, or from the Netherlands, but are not percieved as such, like Al Stankard, Voice of Europe (which merits its own investigation) and Matthew & Doris, that contain a lot of non-political videos.\n",
    "* We should establish which channels are from FvD and run some analysis on them together as a seperate cluster. \n",
    "\n",
    "So let's build some filters first. This code can be used as well if we are going to investigate the Dutch commenters in the international network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What channels do you want to remove from the comment file?\n",
    "\n",
    "to_remove = ['Voice of Europe', 'Matthew & Doris', 'Al Stankard aka HAarlem VEnison']\n",
    "\n",
    "nl_comment_sphere = nl_comment_sphere[~nl_comment_sphere.video_channel_title.isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we're set. Let's look at the prolific commenters first. \n",
    "# Who is commenting a lot in this network in general? \n",
    "\n",
    "topcommenters = nl_comment_sphere.author_display_name.value_counts()\n",
    "topcommenters = topcommenters[0:26]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "ax = topcommenters.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of comments')\n",
    "ax.set_xlabel('name')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are some people that have commented more than 250 times in this dataset\n",
    "\n",
    "Some observations:\n",
    "* There are some channels in there that seem to actively debate with their viewers. I think it's interesting to have a look at the top two, but especially Paul Nielsen for he is affiliated with Forum voor Democratie.\n",
    "* groene hond sounds familiar. I would'nt be suprised if this is the same person as 'botte hond', or 'zilte hond', a notorious social media figure.\n",
    "* The names certainly don't point to real world identities. Yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up: a small group seems responsible for many comments. \n",
    "# Let's do an analysis of the GINI to see if that's true.\n",
    "\n",
    "commenter_groups = nl_comment_sphere.groupby('author_channel_id') #we need these groups later.\n",
    "\n",
    "num_comments = pd.DataFrame(commenter_groups.size().sort_values(ascending = True), columns = ['count'])\n",
    "num_comments['Cumulative percentage of comments'] = 100*num_comments['count'].cumsum()/max(num_comments['count'].cumsum())\n",
    "num_comments['Commenter percentile'] = num_comments.reset_index().index/max(num_comments.reset_index().index)\n",
    "\n",
    "sns.lineplot(x=num_comments['Commenter percentile'],y=num_comments['Cumulative percentage of comments'])\n",
    "\n",
    "del num_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, about 75 percent of the comments are placed by 20 percent of the commenters. And about 50 percent of the comments by about 5 percent of the commenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some are commenting a lot on their own channel (like Paul Nielsen). \n",
    "# Who is commenting all over the place?\n",
    "\n",
    "prolific_commenters = commenter_groups['video_channel_title'].nunique().value_counts()\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "prolific_commenters.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of commenters')\n",
    "ax.set_xlabel('number of channels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far the most commenters (35.000 plus) only comment on 1 dchannel. About 5000 comment on two channels. But we're not interested in these commenters, we want to dive into the tail of this graph, so let's start looking for commenters who are commenting on 5 or more channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_commenters = prolific_commenters[4:]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "prolific_commenters.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of commenters')\n",
    "ax.set_xlabel('number of channels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the names of the most profilic commenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in a threshold of how many different channels someone has been commenting.\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "prolific_commenters = nl_comment_sphere.groupby('author_channel_id') \\\n",
    "                    .filter(lambda x: ((x.video_channel_title.nunique() >= threshold) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most prolific commenters with the number of comments. \n",
    "\n",
    "prolific_commenters_to_plot = prolific_commenters.author_display_name.value_counts()\n",
    "prolific_commenters_to_plot = prolific_commenters_to_plot[0:20]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "prolific_commenters_to_plot.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of commenters')\n",
    "ax.set_xlabel('number of channels')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large overlap between the people who comment a lot and people who comment all over the place. The channels (like politiekincorrecttv and paul nielsen) are gone. If you want to look at the radical core of the Dutch YouTube information network, here it is. Let's explore some of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming in on a couple of persons of interest in the Dutch network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with Peter Chess (would his real name be Peter Schaak?)\n",
    "\n",
    "peter = nl_comment_sphere[nl_comment_sphere['author_display_name'] == 'Peter Chess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be interesting to plot the number of comments per channel on a stacked bar chart. The x-axis is the year. The bar chart consists of channels and the height of the stacked charts the number of comments on those channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = peter.groupby(['video_upload_year','video_channel_title']).size().unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot.area(figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still hard to read, because most of the commenting is after 2012, 2013. So let's start a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p[p.index > 2012] #set the date from where you want the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot.area(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still far from perfect, but it will do for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch commenters on international channels\n",
    "\n",
    "I'm interested in exploring how these (mostly) Dutch users are represented in the larger international far right channel network. So I'll make a list of unique id's and run it through the larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_check = nl_comment_sphere.author_channel_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['video_id', \n",
    "           'comment_id', \n",
    "           'comment_id2', \n",
    "           'author_display_name',\n",
    "           'author_image',\n",
    "           'author_channel_url',\n",
    "           'author_channel_id',\n",
    "           'comment_text',\n",
    "           'number_of_replies',\n",
    "           'comment_date'\n",
    "          ]\n",
    "\n",
    "\n",
    "iter_csv = pd.read_csv(path_i + 'comments_right.csv', iterator=True, chunksize=100000, names=columns)\n",
    "nl_int_comment_sphere = pd.concat([chunk[chunk['author_channel_id'].isin(users_to_check)] for chunk in iter_csv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comments with video and channel data\n",
    "\n",
    "videos_all = pd.read_csv(path_i + 'videos_right.csv', low_memory=False)\n",
    "\n",
    "nl_int_comment_sphere = pd.merge(nl_int_comment_sphere, videos_all, on='video_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many comments do we have in this new dataset?\n",
    "\n",
    "nl_int_comment_sphere['year'] = pd.DatetimeIndex(nl_int_comment_sphere['comment_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is where people who comment on Dutch channels are commenting in our far right network\n",
    "\n",
    "popular_channels_for_dutch = nl_int_comment_sphere.video_channel_title.value_counts()\n",
    "popular_channels_for_dutch = popular_channels_for_dutch[0:20]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "popular_channels_for_dutch.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of comments')\n",
    "ax.set_xlabel('channels')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "* Pat Condell is really popular with Dutch commenters.\n",
    "* Rebel Media is interesting. I didn't know it was that popular.\n",
    "* Millennial Woes scores pretty high as well.\n",
    "\n",
    "Let's gather some more stats on the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commenter_groups = nl_int_comment_sphere.groupby('author_channel_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do another gini analysis (the results are probably the same)\n",
    "\n",
    "num_comments = pd.DataFrame(commenter_groups.size().sort_values(ascending = True), columns = ['count'])\n",
    "num_comments['Cumulative percentage of comments'] = 100*num_comments['count'].cumsum()/max(num_comments['count'].cumsum())\n",
    "num_comments['Commenter percentile'] = num_comments.reset_index().index/max(num_comments.reset_index().index)\n",
    "\n",
    "sns.lineplot(x=num_comments['Commenter percentile'],y=num_comments['Cumulative percentage of comments'])\n",
    "\n",
    "del num_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that the international Dutch comment sphere is a little bit more elitist than the Dutch one, which shouldn't suprise us. \n",
    "\n",
    "Take a look at the number of commenters commenting on n channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_commenters = commenter_groups['video_channel_title'].nunique().value_counts()\n",
    "prolific_commenters = prolific_commenters[0:20]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) \n",
    "\n",
    "width = 0.4\n",
    "\n",
    "prolific_commenters.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of commenters')\n",
    "ax.set_xlabel('number of channels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a little bit more evenly distributed. Let's look at the tail of the graph, so the really prolific commenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_commenters = commenter_groups['video_channel_title'].nunique().value_counts()\n",
    "prolific_commenters = prolific_commenters[40:]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10)) \n",
    "\n",
    "width = 0.4\n",
    "\n",
    "prolific_commenters.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of commenters')\n",
    "ax.set_xlabel('number of channels')\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the people that are really, really prolific, commenting on 39 channels and more. Who are these people? \n",
    "\n",
    "Some observations:\n",
    "\n",
    "1. These are not only Dutch people. No problem. This is something we know, because foreign people can comment on Dutch channels as well. \n",
    "2. There are some interesting people I think. For starters: carolienleiden. Might that be Caroline Dauphine from JFvD? Who knows. Identity Europa is a Dutch guy I think, connected to ID Verzet. There are a couple of Dutch guys calling themselves Pinochet I think, closely related to Erkenbrand and /polder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in a threshold of how many different channels someone has been commenting.\n",
    "\n",
    "threshold = 19\n",
    "threshold2 = 10\n",
    "\n",
    "prolific_commenters = nl_int_comment_sphere.groupby('author_channel_id') \\\n",
    "                    .filter(lambda x: ((x.video_channel_title.nunique() <= threshold) & (x.video_channel_title.nunique() >= threshold2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get all the Dutch sounding names from the group we have filtered out above. In this case, everybody that has commented on Dutch channels and on 10plus channels in the international far right network. And everybody that has been commenting on 10plus channels, in total more than 150 times, in the Dutch network (if we do it less than 150 times, it's a lot of data, plus we want the frequent commenters. 150 seems a nice cut off point, but you can set the bar lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some people with Dutch sounding names \n",
    "# who have commented on more than 10 channels in the international network\n",
    "\n",
    "nl_of_interest_int = ['A Stuijt',\n",
    "                    'Adrie Van Dijk',\n",
    "                    'Akka Fietje',\n",
    "                    'Wouter Lensvelt',\n",
    "                    'Willem Sterk',\n",
    "                    'Michael Groenendijk',\n",
    "                    'Milo Overzicht',\n",
    "                    'Mike De Jong',\n",
    "                    'Mike Brink',\n",
    "                    'Nellie Rutten',\n",
    "                    'Paul van Dijck',\n",
    "                    'Peter Jongsma',\n",
    "                    'Piet Hein',\n",
    "                    'Pieter van der Meer',\n",
    "                    'Polder Cannabis Olie team',\n",
    "                    'Politiekman',\n",
    "                    'Raymond Doetjes',\n",
    "                    'Willem Pasterkamp',\n",
    "                    'Wimpiethe3',\n",
    "                    'Willie van het Kerkhof',\n",
    "                    'Vincent Vermeer',\n",
    "                    'Mark Tak',\n",
    "                    'Melvin Jansen',\n",
    "                    'Mark Kamphuis',\n",
    "                    'Tristan van Oosten',\n",
    "                    'Tom dGe-lugs-pa',\n",
    "                    'Tom Van de Pol',\n",
    "                    'Tom Van Gool',\n",
    "                    'Marcel Bruinsma',\n",
    "                    'Maarten van der Poel',\n",
    "                    'Maciano Van der Laan',\n",
    "                    'Tiemen Weistra',\n",
    "                    'TheRdamterror',\n",
    "                    'TheCitroenman1',\n",
    "                    'The flying dutchman',\n",
    "                    'Teun de Heer',\n",
    "                    'Stijn van de Ven',\n",
    "                    'Sjaak v Koten',\n",
    "                    'Sev Vermeer',\n",
    "                    'Tanya De Beer',\n",
    "                    'Tim Pietersen',\n",
    "                    'Alan Holland',\n",
    "                    'Bennie Leip',\n",
    "                    'Bert Prins',\n",
    "                    'Bestheftig',\n",
    "                    'Borisje Boef',\n",
    "                    'Chris Van Bekkum',\n",
    "                    'Coen Bijpost',\n",
    "                    'Cornelis van der Heijden',\n",
    "                    'David Teunissen',\n",
    "                    'David Van der Tweel',\n",
    "                    'De Veelvraat',\n",
    "                    'Dennis Bouma',\n",
    "                    'Dennis Eijs',\n",
    "                    'Donald gekkehenkie',\n",
    "                    'peter van',\n",
    "                    'onbekende telefoon',\n",
    "                    'nick van achthoven',\n",
    "                    'mikedehoogh black flag race photos',\n",
    "                    'kristof verbruggen',\n",
    "                    'jan holdijk',\n",
    "                    'jan Yup',\n",
    "                    'iwan munnikes',\n",
    "                    'hans van de mortel',\n",
    "                    'geroestetumor',\n",
    "                    'geheimschriver',\n",
    "                    'gaatje niksaan',\n",
    "                    'dutchmountainsnake',\n",
    "                    'dutch menneer',\n",
    "                    'donder bliksem',\n",
    "                    'boereriem',\n",
    "                    'appie D',\n",
    "                    'adam willems',\n",
    "                    'Yuri Klaver',\n",
    "                     'zuigdoos',\n",
    "                    'yvonneforsmanatyahoo',\n",
    "                    'vanhetgoor',\n",
    "                    'theflyingdutchboi',\n",
    "                    'r juttemeijer',\n",
    "                    'rutger houtdijk',\n",
    "                    'Dutch Patriot',\n",
    "                    'Dutch Whitey',\n",
    "                    'DutchFurnace',\n",
    "                    'Esias Lubbe',\n",
    "                    'Ewalds Eiland',\n",
    "                    'Joey Kuijs',\n",
    "                    'Faust',\n",
    "                    'Hollandia777',\n",
    "                    'Johan van Oldenbarnevelt',\n",
    "                    'Keescanadees',\n",
    "                    'Geert Kok',\n",
    "                    'Haasenpad',\n",
    "                    'Henk Damster',\n",
    "                    'Henk van der Laak',\n",
    "                    'Henri Zwols',\n",
    "                    'Haat Praat',\n",
    "                    'Gerard Mulder',    \n",
    "                    'Grootmeester Jan',\n",
    "                    'H. v. Heeswijk',\n",
    "                    'B. Hagen',\n",
    "                    '1234Daan4321',\n",
    "                    'Daniella Thoelen', \n",
    "                    'Diederik',\n",
    "                    'Linda Bostoen', \n",
    "                    'Christiaan Baron', \n",
    "                    'Matthijs van Guilder',\n",
    "                    'Johannes Roose',\n",
    "                    'Deon Van der Westhuizen', \n",
    "                    'Remko Jerphanion', \n",
    "                    'Roosje Keizer',\n",
    "                    'Dennis Durkop',\n",
    "                    'ivar olsen',\n",
    "                    'Pete de pad',\n",
    "                    'georgio jansen',\n",
    "                    'Joel Peter',\n",
    "                    'Antonie de Vry',\n",
    "                    'Stijn Voorhoeve', \n",
    "                    'liefhebber179',\n",
    "                    'Walter Taljaard',\n",
    "                    'joe van gogh',\n",
    "                    'Edo Peter', \n",
    "                    'Ad Lockhorst',\n",
    "                    'kay hoorn',\n",
    "                    'Erik Bottema',\n",
    "                    'Deplorable Data',\n",
    "                    'JESSEverything',\n",
    "                    'Harry Balzak', \n",
    "                    'Bokkepruiker Records',\n",
    "                    'zonnekat',\n",
    "                    'Peter-john De Jong',\n",
    "                    'marco mac',\n",
    "                    'Joubert x',\n",
    "                    'Natasja van Dijk',\n",
    "                    'Voornaam Achternaam',\n",
    "                    'hermanPla', \n",
    "                    'M. van der Scheer',\n",
    "                    'gerald polyak',\n",
    "                    'Robbie Retro',\n",
    "                    'Johannes DeMoravia',\n",
    "                    'Wouter Vos',\n",
    "                    'AwoudeX',\n",
    "                    'carolineleiden',\n",
    "                    'A-dutch-Z',\n",
    "                    'piet ikke',\n",
    "                    'kutbleat',\n",
    "                    'David of Yorkshire',\n",
    "                    'Gert Tjildsen',\n",
    "                    'Flying Dutchman',\n",
    "                    'Visko Van Der Merwe',\n",
    "                    'Blobbejaan Blob',\n",
    "                    'TheBergbok',\n",
    "                    'jknochel76',\n",
    "                    'Olleke Bolleke'\n",
    "                    ]\n",
    "\n",
    "# And top n from nl_most prolific commenters (on 10 Dutch channels or more, with 150 comments or more)\n",
    "\n",
    "nl_of_interest_nl = ['Nayako Sadashi', 'demarcation'  \n",
    "                    'er zaal',\n",
    "                    'jhon jansen',\n",
    "                    '-____-',\n",
    "                    'Brummie Brink',\n",
    "                    'reindeerkid ',\n",
    "                    'Pagan Cloak',\n",
    "                    'NDY',\n",
    "                    'Karel de Kale',\n",
    "                    'top top',\n",
    "                    'Chris Veenendaal ',\n",
    "                    'MijnheerlijkeBuitenlandse befkut ,',\n",
    "                    'Kevin Zilverberg',\n",
    "                    'Rick Dekker ',\n",
    "                    'Adrie Van Dijk ',\n",
    "                    'miep miep',\n",
    "                    'pronto ',\n",
    "                    'TheUnTrustable0',\n",
    "                    'danny schaap',\n",
    "                    'Mark Mathieu',\n",
    "                    'Raysboss302',\n",
    "                    'Ruud Hooreman',\n",
    "                    'Willie W',\n",
    "                    'Barend Borrelworst',\n",
    "                    'theo breytenbach',\n",
    "                     'coinmaster1000 coinmaster1000'  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure, I'm going to run them again through the international network. \n",
    "# You can skip this step if you want.\n",
    "\n",
    "columns = ['video_id', \n",
    "           'comment_id', \n",
    "           'comment_id2', \n",
    "           'author_display_name',\n",
    "           'author_image',\n",
    "           'author_channel_url',\n",
    "           'author_channel_id',\n",
    "           'comment_text',\n",
    "           'number_of_replies',\n",
    "           'comment_date'\n",
    "          ]\n",
    "\n",
    "\n",
    "iter_csv = pd.read_csv(path_i + 'comments_right.csv', iterator=True, chunksize=100000, names=columns)\n",
    "nl_commenters_of_interest = pd.concat([chunk[chunk['author_display_name'].isin(nl_of_interest_int)] for chunk in iter_csv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These people could be the focus of our investigation.\n",
    "\n",
    "nl_commenters_of_interest = pd.merge(nl_commenters_of_interest, videos_all, on='video_id', how='left')\n",
    "\n",
    "nl_commenters_of_interest.to_csv(path + 'nl_commenters_of_interest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nl_commenters_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a couple of them in detail, especially their journey on YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caroline = nl_commenters_of_interest[nl_commenters_of_interest['author_display_name'] == 'carolineleiden']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try an area chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = caroline.groupby(['year','video_channel_title']).size().unstack()\n",
    "c.plot.area(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c[c.index > float(2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c.plot.area(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably better to export the data and have a look at them in [RAWGraphs](http://app.rawgraphs.io/), for instance, in the bump charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = caroline.groupby(['year','video_channel_title']).size()\n",
    "\n",
    "c.to_csv(path + 'for_viz/caronlineleiden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benny = nl_int_comment_sphere[nl_int_comment_sphere['author_display_name'] == 'Bennnnny1987']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = benny.groupby(['year','video_channel_title']).size()\n",
    "\n",
    "b.to_csv(path + 'for_viz/benny.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Follow commenter journeys\n",
    "\n",
    "Okay, let's try something more difficult and follow the commenter's journeys and put that data into a graph. I'll take the prolific commenters as a starting point (start small because this is very memory intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOT 2.3 IS HET NOG WAT PROBEERSELS EN TROEP. DUS SLA DIT EVEN OVER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's zoom in on some channels\n",
    "\n",
    "channel = 'Millennial Woes' #enter the channel name\n",
    "\n",
    "comments_of_interest = int_vid_comments[int_vid_comments['video_channel_title'] == channel]\n",
    "\n",
    "comments_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My impression is that the data are skewed because of Voice of Europe. We can collect more specific data if we want. Let's select one or more Dutch channels first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3: Comparisson\n",
    "\n",
    "Make some comparissons with other information networks, starting with political parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "channels_control = pd.read_csv(path + 'channels_nl_controlgroup_politiek.csv')\n",
    "videos_control = pd.read_csv(path + 'videos_nl_controlgroup_politiek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel and video data from PVV and FvD\n",
    "\n",
    "channel1 = 'Forum Democratie' #fill in the channels you want to compare\n",
    "channel2 = 'PVVpers'\n",
    "\n",
    "pvvfvd_vids = videos.loc[(videos['video_channel_title'] == channel1) | \\\n",
    "                      (videos['video_channel_title'] == channel2)\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvvfvd_channels = channels.loc[(channels['channel_title'] == channel1) | \\\n",
    "                      (channels['channel_title'] == channel2)\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_channels = channels_control.append(pvvfvd_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vids = videos_control.append(pvvfvd_vids, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vids['video_upload_year'] = pd.DatetimeIndex(compare_vids['video_published']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to plot some stuff\n",
    "\n",
    "ax = compare_vids.groupby(['video_upload_year','video_channel_title'])['video_channel_title'] \\\n",
    "        .count().unstack(1).plot.line(title=\"Number of uploaded videos per party\", figsize=(20,10), grid=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of uploads')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the number of uploads per year. I chose a line chart here, because the bar chart is really unclear.\n",
    "# The data are of course discrete and not continuous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_per_year = compare_vids.groupby(['video_upload_year'])['video_view_count'].agg('sum')\n",
    "\n",
    "fig = plt.figure(figsize=(10,5)) # Create matplotlib figure\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "views_per_year.plot(kind='bar', color='red', width=width, grid=True)\n",
    "ax.set_ylabel('number of videos published')\n",
    "ax.set_xlabel('year')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Show the number of views combined per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = compare_vids.groupby(['video_upload_year','video_channel_title'])['video_view_count'] \\\n",
    "        .agg('sum') \\\n",
    "        .unstack(1) \\\n",
    "        .plot \\\n",
    "        .line(title=\"Number of views per party\", \n",
    "            figsize=(20,10), \n",
    "            grid=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of views')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's bit of a mess, but it is way clear that Forum Democratie is outperforming everybody. Maybe it's better to make some decisions on what to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vids = videos_control.append(pvvfvd_vids, sort=True)\n",
    "compare_vids['video_upload_year'] = pd.DatetimeIndex(compare_vids['video_published']).year\n",
    "\n",
    "channels_we_want = ['Forum Democratie', 'Partij van de Arbeid (PvdA)', 'PVVpers', 'DENK TV', 'GroenLinks']\n",
    "\n",
    "compare_vids = compare_vids[compare_vids.video_channel_title.isin(channels_we_want)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = compare_vids.groupby(['video_upload_year','video_channel_title'])['video_view_count'] \\\n",
    "        .agg('sum').unstack(1).plot.line(title=\"Number of views per party\", figsize=(20,10), grid=True)\n",
    "\n",
    "ax.set_xlabel('year')\n",
    "ax.set_ylabel('number of views')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "correspondent",
   "language": "python",
   "name": "correspondent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
