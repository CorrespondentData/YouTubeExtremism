{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top TfIdf words for channels\n",
    "\n",
    "Methodology: similar to https://pudding.cool/2017/09/hip-hop-words/\n",
    "\n",
    "Merge all videos for each channel for every year and see what makes that channel distinctive and if it changes over time.\n",
    "\n",
    "Method:\n",
    "1. Import cleaned captions\n",
    "2. Group them by channel and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "captions = 'C:/hackathon/right_captions.csv'\n",
    "\n",
    "df = pd.read_csv(captions,index_col=0)\n",
    "df['year'] = df['date'].apply(lambda x: x[:4])\n",
    "grouped = df.groupby(['channel','year'])\n",
    "\n",
    "merged = []\n",
    "for group in grouped:\n",
    "    merged.append({'year':group[0][1],'channel_id':list(group[1].unknown)[0], #channel id is called 'unknown' in the csv\n",
    "                   'channel':group[0][0],'text':'\\n'.join(str(v) for v in group[1].content)}) #str if only a number is found\n",
    "merged_df = pd.DataFrame(merged) #run it back to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "###Optional: Lemmatize\n",
    "from spacy.lang.en import English\n",
    "\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)\n",
    "\n",
    "merged_df.text = merged_df.text.apply(lambda x: ' '.join([tok.lemma_ for tok in tokenizer(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf values\n",
    "\n",
    "### Parameter choices\n",
    "Followed the pudding hiphop blog. Terms have to appear in at least one in 50 channels (lower than with the pudding, who use one in 10, because we have a very diverse and large set of channels with topics probably changing a lot over time). Used sublinear term frequency (not 10, but 1 + log(9)), because otherwise stop words appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(min_df=.02,sublinear_tf = True)\n",
    "res = vec.fit_transform(merged_df.text)\n",
    "vocab = {value:key for key,value in vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for index in merged_df.index:\n",
    "    top10words = [vocab[j] for i,j in sorted(zip(res[index].data,res[index].indices),reverse=True)[:10]]\n",
    "    if len(top10words) < 10:\n",
    "        continue\n",
    "    meta = {'year':merged_df.year[index],'channel':merged_df.channel[index],'channel_id':merged_df.channel_id[index]}\n",
    "    words = ({'word{no}'.format(no=i+1):top10words[i] for i in range(10)})\n",
    "    results.append({k: v for d in [meta, words] for k, v in d.items()})\n",
    "top10words_df = pd.DataFrame(results)\n",
    "top10words_df = top10words_df[['year','channel','channel_id']+['word'+str(no) for no in range(1,11)]]\n",
    "top10words_df.to_csv('C:/hackathon/top10tfidf_per_channel.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf top 100 words (for similarity)\n",
    "\n",
    "Parameter choices same as above, but with json output to preserve list structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index in merged_df.index:\n",
    "    top100words = [vocab[j] for i,j in sorted(zip(res[index].data,res[index].indices),reverse=True)[:100]]\n",
    "    if len(top100words) < 100:\n",
    "        continue\n",
    "    results.append({'year':merged_df.year[index],\n",
    "            'channel':merged_df.channel[index],\n",
    "            'channel_id':merged_df.channel_id[index], \n",
    "            'words':top100words})\n",
    "top100words_df = pd.DataFrame(results)\n",
    "top100words_df = top100words_df[['year','channel','channel_id','words']]\n",
    "top100words_df.to_json('C:/hackathon/top100tfidf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Overlap' matrix tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "channel_id = {i:{'year':top100words_df.year[i],\n",
    "                 'channel':top100words_df.channel[i],\n",
    "                 'channel_id':top100words_df.channel_id[i]} for i in top100words_df.index}\n",
    "top100words_df.words = top100words_df.words.apply(set)\n",
    "distance_matrix = np.ones((len(channel_id),len(channel_id)))\n",
    "\n",
    "for i in range(len(channel_id)):\n",
    "    for j in range(len(channel_id)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        elif i > j:\n",
    "            distance_matrix[i,j] = distance_matrix[j,i]\n",
    "        else:\n",
    "            distance_matrix[i,j] = len(top100words_df.words[i] & top100words_df.words[j])/100\n",
    "\n",
    "distance_matrix[distance_matrix < .05] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_numpy_matrix(distance_matrix)\n",
    "\n",
    "for i in range(len(channel_id)):\n",
    "    G.node[i].update(channel_id[i])\n",
    "#nx.write_gexf(G,'C:/hackathon/tfidf_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G,'C:/hackathon/tfidf_graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('C:/hackathon/merged_right.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
