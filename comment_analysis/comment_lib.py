import pandas as pd
import re
import csv
import itertools
import numpy as np
from collections import defaultdict

COMMENT_TEXT_FIELDS = ['video_id', 'comment_id', 'author_display_name',
       'author_channel_url', 'author_channel_id', 'comment_text']
VIDEO_TEXT_FIELDS = ['channel_id', 'video_title', 'video_description',
       'video_channel_title', 'video_default_language', 'video_duration']

VIDEO_NUMERIC_FIELDS = ['video_view_count', 'video_comment_count', 'video_likes_count', 'video_dislikes_count']

def read_comments(comments_file, convert_string = True, convert_numeric = True):
    '''
    Read comments file into Pandas DataFrame.

    Keyword parameters:
    comments_file - filename of comments csv file
    convert_string - boolean. Force conversion of string fields to strings (e.g. comments containing only numbers). 
        Defaults to True
    convert_numeric - boolean. Force conversion to numeric for numeric fields. Defaults to True
    '''
    comments_df = pd.read_csv(comments_file, encoding = 'utf8', header = 0)
    comments_df = comments_df[comments_df['video_id'] != 'video_id']

    comments_df['comment_time'] = pd.to_datetime(comments_df['comment_time'])

    if convert_numeric:
        comments_df['comment_like_count'] = pd.to_numeric(comments_df['comment_like_count'])

    if convert_string:
        for field in COMMENT_TEXT_FIELDS:
            comments_df[field] = comments_df[field].apply(str)

    return comments_df

def add_video_metadata(comments_df, video_file, channel_file = None, keep_vars = None, convert_string = True, convert_numeric = True):
    '''
    Add video metadata to the comments dataset

    Keyword parameters:
    comments_df - DataFrame with comments, construct using comment_lib.read_comments
    video_file - filename of video metadata csv file
    channel_file - filename of channel metadata csv file for channel names, defaults to None (ignore)
    keep_vars - list of variables to keep (a selection of video variables). If not supplied, all will be matched.
        video_id is always kept to match, does not necessarily need to be specified here.
    convert_string - boolean. Force conversion of string fields to strings (e.g. comments containing only numbers). 
        Defaults to True
    convert_numeric - boolean. Force conversion to numeric for numeric fields. Defaults to True
    '''
    video_df = pd.read_csv(video_file, encoding = 'utf8', header = 0)
    video_df = video_df[video_df['video_id'] != 'video_id']

    if convert_string:
        for field in VIDEO_TEXT_FIELDS:
            video_df[field] = video_df[field].apply(str)
    if convert_numeric:
        for field in VIDEO_NUMERIC_FIELDS:
            video_df[field] = pd.to_numeric(video_df[field], errors = 'coerce')

    video_df['video_published'] = pd.to_datetime(video_df['video_published'])
    
    #Set duration to integer (number of seconds)
    video_df['video_duration'] = video_df['video_duration'].apply(video_duration_to_int)

    if keep_vars:
        if 'video_id' not in keep_vars:
            keep_vars.append('video_id')
        video_df = video_df[keep_vars]

    output = comments_df.merge(video_df, how = 'left', on = 'video_id')

    if channel_file:
        channel_df = pd.read_csv(channel_file, encoding = 'utf8', header = 0)[['channel_id','channel_title']]
        output = output.merge(channel_df, on = 'channel_id', how = 'left')

    return output

def video_duration_minutes(duration_string):
    ''' Get the number of minutes from a youtube video duration string'''
    minute_count = re.search('(?<=PT)[0-9]+(?=M)', duration_string)
    if minute_count:
        return int(minute_count[0])
    else:
        return 0

def video_duration_seconds(duration_string):
    ''' Get the number of seconds from a youtube video duration string'''
    second_count = re.search('(?<=M)[0-9]+(?=S)', duration_string)
    if second_count:
        return int(second_count[0])
    else:
        return 0

def video_duration_to_int(duration_string):
    '''Convert a youtube video duration string to the total number of seconds'''
    return 60*video_duration_minutes(duration_string) + video_duration_seconds(duration_string)

def generate_network_comments_aggregated(comments_df, out_file, nodes = 'channel', minimum_connections = 1):
    '''
    Generate edges between channels if they share a commenter and aggregates a count, save as csv

    Keyword arguments:
    comments_df -- DataFrame with comments and channel_id, generated by comment_lib
    out_file -- filename to save edges
    nodes -- str in {'channel','video'}
    minimum_connections -- minimum number for inclusion. Defaults to 1
    '''

    if nodes not in {'channel','video'}:
        raise Exception()
    
    if nodes == 'channel':
        node_var = 'channel_id'
    else:
        node_var = 'video_id'

    commenter_groups = comments_df.groupby('author_channel_id')

    author_channels = commenter_groups[node_var].apply(set).to_dict()
    author_channels = {k:remove_nan_set(v) for k,v in author_channels.items()}
    author_channels = {k:v for k,v in author_channels.items() if len(v) != 0}

    edges = defaultdict(lambda:0)

    for _, channels in author_channels.items():
        if len(channels) == 1:
            continue
        
        for source, target in itertools.combinations(channels,2):
            edges[(source,target)] += 1

    with open(out_file, 'w', encoding = 'utf8') as csvfile:
        writer = csv.DictWriter(csvfile, lineterminator='\n', fieldnames=['source','target','weight'])

        writer.writeheader()
        for keys,value in edges.items():
            if value >= minimum_connections:
                writer.writerow({'source':keys[0],'target':keys[1],'weight':value})

def generate_network_comments_with_users(comments_df, out_file, nodes = 'channel'):
    '''
    Generate edges between channels if they share a commenter, save as csv

    Keyword arguments:
    comments_df -- DataFrame with comments and channel_id, generated by comment_lib
    out_file -- filename to save edges
    nodes -- str in {'channel','video'}
    '''

    if nodes not in {'channel','video'}:
        raise Exception()
    
    if nodes == 'channel':
        node_var = 'channel_id'
    else:
        node_var = 'video_id'

    commenter_groups = comments_df.groupby('author_channel_id')

    author_channels = commenter_groups[node_var].apply(set).to_dict()
    author_channels = {k:remove_nan_set(v) for k,v in author_channels.items()}
    author_channels = {k:v for k,v in author_channels.items() if len(v) != 0}

    edges = []
    for author, channels in author_channels.items():
        if len(channels) == 1:
            continue
        
        for source, target in itertools.combinations(channels,2):
            edges.append({'source':source,'target':target,'user':author,'weight':1.0})

    with open(out_file, 'w', encoding = 'utf8') as csvfile:
        writer = csv.DictWriter(csvfile, lineterminator='\n', fieldnames=edges[0].keys())

        writer.writeheader()
        for row in edges:
            writer.writerow(row)

def remove_nan_set(data):
    ''' Remove np.nan from a set (defined for use in dict comprehensions)'''
    if np.nan in data:
        data.remove(np.nan)
    return data